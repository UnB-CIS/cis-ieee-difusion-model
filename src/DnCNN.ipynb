{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "C0bD9OmC2qb1"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'onedrive_client'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     18\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monedrive\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monedrive_dncnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m authenticate_onedrive, download_and_process_in_batches\n",
            "File \u001b[0;32m~/Documents/cis/cis-ieee-difusion-model/src/data/onedrive/onedrive_dncnn.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01monedrive_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OneDriveClient\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mauthenticate_onedrive\u001b[39m(interactive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    Authenticate with OneDrive using the OneDriveClient.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onedrive_client'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "import sys\n",
        "sys.path.append('.')  \n",
        "\n",
        "from src.data.onedrive.onedrive_dncnn import authenticate_onedrive, download_and_process_in_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD4D3ibB3bbb"
      },
      "outputs": [],
      "source": [
        "\n",
        "client = authenticate_onedrive(interactive=True)\n",
        "\n",
        "load_dotenv()\n",
        "folder_id = os.getenv('FOLDER_ID')\n",
        "if not folder_id:\n",
        "    raise ValueError(\"FOLDER_ID not set in environment variables\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and process images in batches\n",
        "download_and_process_in_batches(client, folder_id, target_dir='images', batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJUJaDZq_yIi"
      },
      "outputs": [],
      "source": [
        "def split_images(image_folder, train_ratio=0.8, test_ratio=0.2,seed=42):\n",
        "    \"\"\"Splits images into train and test sets.\"\"\"\n",
        "\n",
        "    if not os.path.exists(image_folder):\n",
        "        print(f\"Error: Image folder '{image_folder}' not found.\")\n",
        "        return\n",
        "\n",
        "    images = [img for img in os.listdir(image_folder) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    num_images = len(images)\n",
        "    if num_images == 0:\n",
        "        print(f\"Error: No images found in '{image_folder}'.\")\n",
        "        return\n",
        "    random.seed(seed)\n",
        "    random.shuffle(images)  # Shuffle images randomly\n",
        "\n",
        "    # Calculate split indices\n",
        "    train_split = int(num_images * train_ratio)\n",
        "    test_split = int(num_images * (train_ratio + test_ratio))\n",
        "\n",
        "    # Create train and test directories\n",
        "    train_dir = os.path.join(image_folder, \"train\")\n",
        "    test_dir = os.path.join(image_folder, \"test\")\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    # Move images to respective directories\n",
        "    for i, image in enumerate(images):\n",
        "        source_path = os.path.join(image_folder, image)\n",
        "        if i < train_split:\n",
        "            destination_path = os.path.join(train_dir, image)\n",
        "        elif i < test_split:\n",
        "            destination_path = os.path.join(test_dir, image)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        shutil.move(source_path, destination_path)\n",
        "\n",
        "    print(f\"Images split into train ({train_split} images) and test ({test_split-train_split} images) sets.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHFiYCSL2ioa"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(folder_path, noise_std=0.1):\n",
        "    \"\"\"\n",
        "    Prepara o dataset para treinamento adicionando ruído às imagens.\n",
        "    \"\"\"\n",
        "    def load_and_preprocess_image(path):\n",
        "        # Carregar e preprocessar imagem\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.image.decode_png(img, channels=3)\n",
        "        img = tf.cast(img, tf.float32) / 255.0  # Normalizar para [0,1]\n",
        "\n",
        "        # Adicionar ruído gaussiano\n",
        "        noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=noise_std)\n",
        "        noisy_img = img + noise\n",
        "\n",
        "        # Clipar valores para manter entre [0,1]\n",
        "        noisy_img = tf.clip_by_value(noisy_img, 0.0, 1.0)\n",
        "\n",
        "        return noisy_img, img\n",
        "\n",
        "    # Listar caminhos de todas as imagens na pasta\n",
        "    image_paths = [os.path.join(folder_path, fname) for fname in os.listdir(folder_path) if fname.endswith('.png')]\n",
        "\n",
        "    # Criar dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(4)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuDACNkY2W4Q"
      },
      "outputs": [],
      "source": [
        "class DnCNN(Model):\n",
        "    def __init__(self, D, C=64):\n",
        "        super(DnCNN, self).__init__()\n",
        "        self.D = D\n",
        "\n",
        "        # Convolution layers\n",
        "        self.conv_layers = []\n",
        "        self.conv_layers.append(layers.Conv2D(C, kernel_size=3, padding='same', activation=None, input_shape=(None, None, 3)))\n",
        "\n",
        "        for _ in range(D):\n",
        "            self.conv_layers.append(layers.Conv2D(C, kernel_size=3, padding='same', activation=None))\n",
        "\n",
        "        self.conv_layers.append(layers.Conv2D(3, kernel_size=3, padding='same', activation=None))\n",
        "\n",
        "        # Batch normalization layers\n",
        "        self.bn_layers = [layers.BatchNormalization() for _ in range(D)]\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        h = tf.nn.relu(self.conv_layers[0](x))\n",
        "        for i in range(self.D):\n",
        "            h = tf.nn.relu(self.bn_layers[i](self.conv_layers[i+1](h), training=training))\n",
        "        y = self.conv_layers[self.D+1](h) + x\n",
        "        return y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN7MfDxO_7gr",
        "outputId": "60eeca57-0189-4a40-e736-844674fdf1c5"
      },
      "outputs": [],
      "source": [
        "image_folder = \"images\"\n",
        "split_images(image_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc2uf7d__RM4"
      },
      "outputs": [],
      "source": [
        "train_dataset = prepare_dataset('images/train/', noise_std=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NthjwSr4A4eS",
        "outputId": "582510e1-903f-4a6b-c8a3-eebcb304601f"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5cDZdpS2e4a"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dataset, epochs=200, learning_rate=1e-3):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "    model.fit(train_dataset, epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "6eu9o683Blia",
        "outputId": "1afcf383-85ea-4a90-d623-0d210fe9df14"
      },
      "outputs": [],
      "source": [
        "for noisy_images, noise in train_dataset.take(1):\n",
        "    for i in range(min(5, noisy_images.shape[0])):\n",
        "        noisy_image = noisy_images[i].numpy()\n",
        "        plt.figure(figsize=(4, 4))\n",
        "        plt.imshow(noisy_image)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Noisy Image {i + 1}\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaG1GYtn_Usi",
        "outputId": "9343b7ad-1c99-4d0f-e171-23639da0f952"
      },
      "outputs": [],
      "source": [
        "model = DnCNN(D=10)\n",
        "history = train_dncnn_with_onedrive(\n",
        "    model, \n",
        "    folder_id, \n",
        "    client=client, \n",
        "    batch_size=10,  # Adjust based on your memory constraints\n",
        "    epochs=10\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU5SlYcm3zXM"
      },
      "outputs": [],
      "source": [
        "def test_network(model, image_path, noise_std=0.1):\n",
        "    # Load and preprocess the image\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.resize(img, (128, 128))\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    # Add Gaussian noise\n",
        "    noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=noise_std)\n",
        "    noisy_img = img + noise\n",
        "    noisy_img = tf.clip_by_value(noisy_img, 0.0, 1.0)  # Keep values in [0, 1]\n",
        "\n",
        "    # Predict noise using the model\n",
        "    denoised_img = model(noisy_img, training=False)\n",
        "\n",
        "    # Denoise the image by subtracting the predicted noise\n",
        "    denoised_img = tf.clip_by_value(denoised_img, 0.0, 1.0)\n",
        "\n",
        "    # Visualize the results\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.imshow(tf.squeeze(img).numpy())\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.imshow(tf.squeeze(noisy_img).numpy())\n",
        "    plt.title(\"Noisy Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.imshow(tf.squeeze(denoised_img).numpy())\n",
        "    plt.title(\"Denoised Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "    return (img, noisy_img, denoised_img)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_AMEwXaIeE8"
      },
      "outputs": [],
      "source": [
        "def apply_median_filter(result, kernel_size=3):\n",
        "    # Convert TensorFlow tensor to numpy array\n",
        "    img = tf.squeeze(result[0]).numpy()\n",
        "    noisy_img = tf.squeeze(result[1]).numpy()\n",
        "\n",
        "\n",
        "    # Apply median filter\n",
        "    denoised_img = np.zeros_like(noisy_img)\n",
        "    for i in range(3):  # Apply filter to each channel independently\n",
        "        denoised_img[:, :, i] = cv2.medianBlur(noisy_img[:, :, i], kernel_size)\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(2, 3, 4)\n",
        "    plt.imshow(tf.squeeze(img).numpy())\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(2, 3, 5)\n",
        "    plt.imshow(tf.squeeze(noisy_img).numpy())\n",
        "    plt.title(\"Noisy Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(2, 3, 6)\n",
        "    plt.imshow(tf.squeeze(denoised_img).numpy())\n",
        "    plt.title(\"Denoised Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "amdHj5zZ4ghk",
        "outputId": "3109c60e-252f-4fcc-992d-be4a0ccfb656"
      },
      "outputs": [],
      "source": [
        "# Test the network on a sample image\n",
        "result = test_network(model, \"images/test/image_138.png\", noise_std=0.1)\n",
        "median = apply_median_filter(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6qWTV-s7bYV"
      },
      "outputs": [],
      "source": [
        "model.save('/content/weights.keras')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
