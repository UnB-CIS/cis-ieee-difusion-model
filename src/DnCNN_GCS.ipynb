{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "C0bD9OmC2qb1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "import gc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "eRbbbItte8Jt"
      },
      "outputs": [],
      "source": [
        "project_id = 'axial-glow-456914-n5'\n",
        "\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yBjejGSZilc4"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "storage_client = storage.Client(project=project_id)\n",
        "\n",
        "bucket_name = 'cis-difusion-dataset'\n",
        "bucket = storage_client.bucket(bucket_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwO1VBkgSpKW"
      },
      "outputs": [],
      "source": [
        "def check_folder_contents(bucket, folder_name):\n",
        "    \"\"\"\n",
        "    Checks and lists files inside a specific folder in a GCS bucket.\n",
        "\n",
        "    Args:\n",
        "        bucket: The GCS bucket object\n",
        "        folder_name: The name of the folder to check\n",
        "\n",
        "    Returns:\n",
        "        A list of file names in the folder\n",
        "    \"\"\"\n",
        "    # Make sure the folder name ends with a slash\n",
        "    if not folder_name.endswith('/'):\n",
        "        folder_name += '/'\n",
        "\n",
        "    # List blobs with the specified prefix\n",
        "    blobs = list(bucket.list_blobs(prefix=folder_name))\n",
        "\n",
        "    # Filter out the folder itself\n",
        "    files = [blob.name for blob in blobs if blob.name != folder_name]\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Checking folder: {folder_name}\")\n",
        "\n",
        "    if files:\n",
        "        print(f\"Found {len(files)} files in the folder:\")\n",
        "        for file in files:\n",
        "            print(f\"- {file}\")\n",
        "    else:\n",
        "        print(f\"The folder '{folder_name}' is empty or doesn't exist.\")\n",
        "\n",
        "    return files\n",
        "\n",
        "# Example usage:\n",
        "check_folder_contents(bucket, \"DIV2K_train_HR\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "collapsed": true,
        "id": "9mVmtiz8jtX7"
      },
      "outputs": [],
      "source": [
        "folder_name = 'DIV2K_train_HR'  # If you want to download a specific folder\n",
        "local_path = '/dataset/'  # Local path to save the downloaded data\n",
        "\n",
        "def download_files_from_gcs(bucket_name, folder_name, local_path):\n",
        "    \"\"\"\n",
        "    Downloads files from a Google Cloud Storage folder to a local directory.\n",
        "    \"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    os.makedirs(local_path, exist_ok=True)\n",
        "\n",
        "    blobs = bucket.list_blobs(prefix=folder_name)\n",
        "    for blob in blobs:\n",
        "        if not blob.name.endswith('/'):\n",
        "            file_name = os.path.basename(blob.name)\n",
        "            blob.download_to_filename(os.path.join(local_path, file_name))\n",
        "            print(f'Downloaded: {blob.name} to {os.path.join(local_path, file_name)}')\n",
        "\n",
        "    print('Download complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "gOXiG41hvB54"
      },
      "outputs": [],
      "source": [
        "def download_image_batch(bucket, folder_name, local_path, batch_size=100, start_index=0):\n",
        "    \"\"\"Downloads a batch of images from Google Cloud Storage.\"\"\"\n",
        "    # Create local directory if it doesn't exist\n",
        "    os.makedirs(local_path, exist_ok=True)\n",
        "\n",
        "    blobs = list(bucket.list_blobs(prefix=folder_name))\n",
        "    image_paths = []\n",
        "    count = 0\n",
        "\n",
        "    for i, blob in enumerate(blobs):\n",
        "        if i < start_index:\n",
        "            continue  # Skip already downloaded images\n",
        "        if not blob.name.endswith('/') and count < batch_size:\n",
        "            file_name = os.path.basename(blob.name)\n",
        "            local_file_path = os.path.join(local_path, file_name)\n",
        "            blob.download_to_filename(local_file_path)\n",
        "            image_paths.append(local_file_path)  # Ensure this is a string\n",
        "            count += 1\n",
        "            print(f'Downloaded: {blob.name} to {local_file_path}')\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print(f'Downloaded {count} images.')\n",
        "    return image_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "EHFiYCSL2ioa"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(bucket_name, folder_name, noise_std=0.1, img_size=(128, 128), is_training=True):  # Added is_training flag\n",
        "    \"\"\"\n",
        "    Prepares the dataset for training by adding noise to the images.\n",
        "    \"\"\"\n",
        "    def load_and_preprocess_image(path):\n",
        "        # Load and preprocess image\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.image.decode_png(img, channels=3)\n",
        "        # Resize the image\n",
        "        img = tf.image.resize(img, img_size)\n",
        "        img = tf.cast(img, tf.float32) / 255.0  # Normalize to [0,1]\n",
        "\n",
        "        if is_training:\n",
        "            # Add Gaussian noise\n",
        "            noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=noise_std)\n",
        "            noisy_img = img + noise\n",
        "\n",
        "            # Clip values to keep between [0,1]\n",
        "            noisy_img = tf.clip_by_value(noisy_img, 0.0, 1.0)\n",
        "\n",
        "            return noisy_img, img  # Return noisy and original for training\n",
        "        else:\n",
        "            return img, img  # Return original twice for testing\n",
        "\n",
        "    # List paths of all images in the folder\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blobs = bucket.list_blobs(prefix=folder_name)\n",
        "    image_paths = [f'gs://{bucket_name}/{blob.name}' for blob in blobs if not blob.name.endswith('/')]\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(4)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "3TeD-PuyRI-M"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset_local(local_image_paths, noise_std=0.1, img_size=(128, 128), is_training=True):\n",
        "    \"\"\"\n",
        "    Prepara o dataset para treinamento usando imagens locais baixadas.\n",
        "    \"\"\"\n",
        "    # First, ensure all paths are strings\n",
        "    local_image_paths = [str(path) for path in local_image_paths]\n",
        "\n",
        "    def load_and_preprocess_image(path):\n",
        "        # Ensure path is a string (TensorFlow operations might convert it)\n",
        "        path = tf.convert_to_tensor(path, dtype=tf.string)\n",
        "\n",
        "        # Carregar e pré-processar imagem\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.image.decode_png(img, channels=3)\n",
        "        # Redimensionar a imagem\n",
        "        img = tf.image.resize(img, img_size)\n",
        "        img = tf.cast(img, tf.float32) / 255.0  # Normalizar para [0,1]\n",
        "\n",
        "        if is_training:\n",
        "            # Adicionar ruído Gaussiano\n",
        "            noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=noise_std)\n",
        "            noisy_img = img + noise\n",
        "\n",
        "            # Limitar valores para manter entre [0,1]\n",
        "            noisy_img = tf.clip_by_value(noisy_img, 0.0, 1.0)\n",
        "\n",
        "            return noisy_img, img  # Retornar ruidosa e original para treinamento\n",
        "        else:\n",
        "            return img, img  # Retornar original duas vezes para teste\n",
        "\n",
        "    # Criar dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(local_image_paths)\n",
        "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(4)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "CuDACNkY2W4Q"
      },
      "outputs": [],
      "source": [
        "class DnCNN(Model):\n",
        "    def __init__(self, D, C=64):\n",
        "        super(DnCNN, self).__init__()\n",
        "        self.D = D\n",
        "        # Create convolution layers\n",
        "        self.conv_layers = [layers.Conv2D(C, kernel_size=3, padding='same', input_shape=(None, None, 3))]\n",
        "        self.conv_layers.extend([layers.Conv2D(C, kernel_size=3, padding='same') for _ in range(D)])\n",
        "        self.conv_layers.append(layers.Conv2D(3, kernel_size=3, padding='same'))\n",
        "        # BatchNormalization doesn't take an activation parameter\n",
        "        self.bn_layers = [layers.BatchNormalization() for _ in range(D)]\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        h = tf.nn.relu(self.conv_layers[0](x))\n",
        "        for i in range(self.D):\n",
        "            # Apply batch normalization\n",
        "            h = self.bn_layers[i](self.conv_layers[i + 1](h), training=training)\n",
        "            # Apply ReLU activation separately\n",
        "            h = tf.nn.relu(h)\n",
        "        y = self.conv_layers[-1](h) + x\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "_5cDZdpS2e4a"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_model(model, train_dataset, epochs=200, learning_rate=1e-3):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "    model.fit(train_dataset, epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Ya6pHczV1QSG"
      },
      "outputs": [],
      "source": [
        "def train_model_in_batches(model, bucket, train_folder_name, local_path, batch_size=100, epochs_per_batch=10):\n",
        "    \"\"\"Treina o modelo em batches de imagens baixadas do GCS.\"\"\"\n",
        "    num_batches = 5  # Você pode ajustar isso com base no número desejado de batches\n",
        "\n",
        "    for batch_index in range(num_batches):\n",
        "        print(f\"Treinando no batch {batch_index + 1}/{num_batches}\")\n",
        "\n",
        "        # Baixar um batch de imagens do conjunto de treinamento\n",
        "        start_index = batch_index * batch_size  # Calcular índice inicial para download\n",
        "\n",
        "        # Usar train_folder_name para treinamento\n",
        "        image_paths = download_image_batch(bucket, train_folder_name, local_path, batch_size, start_index)\n",
        "\n",
        "        # Preparar dataset para o batch atual usando as imagens baixadas\n",
        "        train_dataset = prepare_dataset_local(image_paths, noise_std=0.1, img_size=(128, 128), is_training=True)\n",
        "\n",
        "        # Treinar o modelo para o batch atual\n",
        "        train_model(model, train_dataset, epochs=epochs_per_batch)\n",
        "\n",
        "        # Salvar pesos após cada batch\n",
        "        model.save_weights('/content/weights.weights.h5')\n",
        "        print(f\"Treinamento do batch {batch_index + 1} completo. Pesos salvos.\")\n",
        "\n",
        "        # Apagar imagens baixadas para liberar espaço\n",
        "        shutil.rmtree(local_path)\n",
        "        os.makedirs(local_path, exist_ok=True)\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Gkhyg7xZvlJo"
      },
      "outputs": [],
      "source": [
        "def test_network_batch(model, bucket_name, test_folder_name, noise_std=0.1, batch_size=4):\n",
        "    \"\"\"Tests the model on a batch of images loaded from GCS.\"\"\"\n",
        "\n",
        "    # Load the saved weights into the model\n",
        "    if os.path.exists('/content/weights.weights.h5'):  # Check if weights file exists\n",
        "        model.load_weights('/content/weights.weights.h5')\n",
        "        print(\"Loaded saved weights for testing.\")\n",
        "    else:\n",
        "        print(\"Warning: Weights file not found. Using untrained model for testing.\")\n",
        "\n",
        "    # Definir image_paths antes de usá-lo\n",
        "    local_path = '/content/test_dataset/'\n",
        "    os.makedirs(local_path, exist_ok=True)\n",
        "    image_paths = download_image_batch(storage_client.bucket(bucket_name), test_folder_name, local_path, batch_size)\n",
        "\n",
        "    # Preparar o dataset de teste\n",
        "    test_dataset = prepare_dataset_local(image_paths, is_training=False, noise_std=noise_std, img_size=(128, 128))\n",
        "\n",
        "    for noisy_images, clean_images in test_dataset.take(1):\n",
        "        denoised_images = model(noisy_images, training=False)\n",
        "\n",
        "        for i in range(min(5, noisy_images.shape[0])):\n",
        "            # Visualize the results\n",
        "            plt.figure(figsize=(15, 5))\n",
        "\n",
        "            plt.subplot(1, 3, 1)\n",
        "            plt.imshow(tf.squeeze(clean_images[i]).numpy())\n",
        "            plt.title(\"Original Image\")\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            plt.subplot(1, 3, 2)\n",
        "            plt.imshow(tf.squeeze(noisy_images[i]).numpy())\n",
        "            plt.title(\"Noisy Image\")\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            plt.subplot(1, 3, 3)\n",
        "            plt.imshow(tf.squeeze(denoised_images[i]).numpy())\n",
        "            plt.title(\"Denoised Image\")\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "RU5SlYcm3zXM"
      },
      "outputs": [],
      "source": [
        "def test_network(model, image_path, noise_std=0.1):\n",
        "    # Load and preprocess the image\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.resize(img, (128, 128))\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    # Add Gaussian noise\n",
        "    noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=noise_std)\n",
        "    noisy_img = img + noise\n",
        "    noisy_img = tf.clip_by_value(noisy_img, 0.0, 1.0)  # Keep values in [0, 1]\n",
        "\n",
        "    # Predict noise using the model\n",
        "    denoised_img = model(noisy_img, training=False)\n",
        "\n",
        "    # Denoise the image by subtracting the predicted noise\n",
        "    denoised_img = tf.clip_by_value(denoised_img, 0.0, 1.0)\n",
        "\n",
        "    # Visualize the results\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.imshow(tf.squeeze(img).numpy())\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.imshow(tf.squeeze(noisy_img).numpy())\n",
        "    plt.title(\"Noisy Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.imshow(tf.squeeze(denoised_img).numpy())\n",
        "    plt.title(\"Denoised Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "    return (img, noisy_img, denoised_img)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3ceUcSRf54Yl",
        "outputId": "2daab08c-fafc-4968-868a-3a35450f1c2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Treinando no batch 1/5\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0001.png to /content/dataset/0001.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0002.png to /content/dataset/0002.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0003.png to /content/dataset/0003.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0004.png to /content/dataset/0004.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0005.png to /content/dataset/0005.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0006.png to /content/dataset/0006.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0007.png to /content/dataset/0007.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0008.png to /content/dataset/0008.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0009.png to /content/dataset/0009.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0010.png to /content/dataset/0010.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0011.png to /content/dataset/0011.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0012.png to /content/dataset/0012.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0013.png to /content/dataset/0013.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0014.png to /content/dataset/0014.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0015.png to /content/dataset/0015.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0016.png to /content/dataset/0016.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0017.png to /content/dataset/0017.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0018.png to /content/dataset/0018.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0019.png to /content/dataset/0019.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0020.png to /content/dataset/0020.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0021.png to /content/dataset/0021.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0022.png to /content/dataset/0022.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0023.png to /content/dataset/0023.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0024.png to /content/dataset/0024.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0025.png to /content/dataset/0025.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0026.png to /content/dataset/0026.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0027.png to /content/dataset/0027.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0028.png to /content/dataset/0028.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0029.png to /content/dataset/0029.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0030.png to /content/dataset/0030.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0031.png to /content/dataset/0031.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0032.png to /content/dataset/0032.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0033.png to /content/dataset/0033.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0034.png to /content/dataset/0034.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0035.png to /content/dataset/0035.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0036.png to /content/dataset/0036.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0037.png to /content/dataset/0037.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0038.png to /content/dataset/0038.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0039.png to /content/dataset/0039.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0040.png to /content/dataset/0040.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0041.png to /content/dataset/0041.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0042.png to /content/dataset/0042.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0043.png to /content/dataset/0043.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0044.png to /content/dataset/0044.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0045.png to /content/dataset/0045.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0046.png to /content/dataset/0046.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0047.png to /content/dataset/0047.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0048.png to /content/dataset/0048.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0049.png to /content/dataset/0049.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0050.png to /content/dataset/0050.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0051.png to /content/dataset/0051.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0052.png to /content/dataset/0052.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0053.png to /content/dataset/0053.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0054.png to /content/dataset/0054.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0055.png to /content/dataset/0055.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0056.png to /content/dataset/0056.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0057.png to /content/dataset/0057.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0058.png to /content/dataset/0058.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0059.png to /content/dataset/0059.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0060.png to /content/dataset/0060.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0061.png to /content/dataset/0061.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0062.png to /content/dataset/0062.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0063.png to /content/dataset/0063.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0064.png to /content/dataset/0064.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0065.png to /content/dataset/0065.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0066.png to /content/dataset/0066.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0067.png to /content/dataset/0067.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0068.png to /content/dataset/0068.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0069.png to /content/dataset/0069.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0070.png to /content/dataset/0070.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0071.png to /content/dataset/0071.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0072.png to /content/dataset/0072.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0073.png to /content/dataset/0073.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0074.png to /content/dataset/0074.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0075.png to /content/dataset/0075.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0076.png to /content/dataset/0076.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0077.png to /content/dataset/0077.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0078.png to /content/dataset/0078.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0079.png to /content/dataset/0079.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0080.png to /content/dataset/0080.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0081.png to /content/dataset/0081.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0082.png to /content/dataset/0082.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0083.png to /content/dataset/0083.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0084.png to /content/dataset/0084.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0085.png to /content/dataset/0085.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0086.png to /content/dataset/0086.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0087.png to /content/dataset/0087.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0088.png to /content/dataset/0088.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0089.png to /content/dataset/0089.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0090.png to /content/dataset/0090.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0091.png to /content/dataset/0091.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0092.png to /content/dataset/0092.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0093.png to /content/dataset/0093.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0094.png to /content/dataset/0094.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0095.png to /content/dataset/0095.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0096.png to /content/dataset/0096.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0097.png to /content/dataset/0097.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0098.png to /content/dataset/0098.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0099.png to /content/dataset/0099.png\n",
            "Downloaded: DIV2K_train_HR/DIV2K_train_HR/DIV2K_train_HR/0100.png to /content/dataset/0100.png\n",
            "Downloaded 100 images.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "in user code:\n\n    File \"<ipython-input-43-319b02983d4b>\", line 7, in train_model  *\n        model.fit(train_dataset, epochs=epochs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/tf_dataset_adapter.py\", line 68, in num_batches\n        cardinality = int(self._dataset.cardinality())\n\n    TypeError: int() argument must be a string, a bytes-like object or a real number, not 'SymbolicTensor'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-3c9d5fbb861d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Treinar o modelo em batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_model_in_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_folder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_per_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Testar o modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-8238222d29c4>\u001b[0m in \u001b[0;36mtrain_model_in_batches\u001b[0;34m(model, bucket, train_folder_name, local_path, batch_size, epochs_per_batch)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Treinar o modelo para o batch atual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_per_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Salvar pesos após cada batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_fileckjzipzv.py\u001b[0m in \u001b[0;36mtf__train_model\u001b[0;34m(model, train_dataset, epochs, learning_rate)\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf__train_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/tf_dataset_adapter.py\u001b[0m in \u001b[0;36mnum_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcardinality\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# `dataset.cardinality` is normally expected to be a callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mcardinality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# However, in the case of `DistributedDataset`, it's a np.int64.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"<ipython-input-43-319b02983d4b>\", line 7, in train_model  *\n        model.fit(train_dataset, epochs=epochs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/tf_dataset_adapter.py\", line 68, in num_batches\n        cardinality = int(self._dataset.cardinality())\n\n    TypeError: int() argument must be a string, a bytes-like object or a real number, not 'SymbolicTensor'\n"
          ]
        }
      ],
      "source": [
        "# Definir parâmetros\n",
        "model = DnCNN(D=8)\n",
        "train_folder_name = f\"{folder_name}/DIV2K_train_HR\"  # Nome da pasta de treinamento\n",
        "local_path = '/content/dataset/'  # Caminho local para salvar os dados baixados\n",
        "os.makedirs('/content/dataset/', exist_ok=True)\n",
        "batch_size = 100  # Tamanho do batch\n",
        "epochs_per_batch = 10  # Épocas por batch\n",
        "\n",
        "# Treinar o modelo em batches\n",
        "train_model_in_batches(model, bucket, train_folder_name, local_path, batch_size, epochs_per_batch)\n",
        "\n",
        "# Testar o modelo\n",
        "test_folder_name = f\"{folder_name}/DIV2K_valid_HR\"  # Nome da pasta de teste\n",
        "result = test_network_batch(model, bucket_name, test_folder_name, noise_std=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_AMEwXaIeE8"
      },
      "outputs": [],
      "source": [
        "def apply_median_filter(result, kernel_size=3):\n",
        "    # Convert TensorFlow tensor to numpy array\n",
        "    img = tf.squeeze(result[0]).numpy()\n",
        "    noisy_img = tf.squeeze(result[1]).numpy()\n",
        "\n",
        "\n",
        "    # Apply median filter\n",
        "    denoised_img = np.zeros_like(noisy_img)\n",
        "    for i in range(3):  # Apply filter to each channel independently\n",
        "        denoised_img[:, :, i] = cv2.medianBlur(noisy_img[:, :, i], kernel_size)\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(2, 3, 4)\n",
        "    plt.imshow(tf.squeeze(img).numpy())\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(2, 3, 5)\n",
        "    plt.imshow(tf.squeeze(noisy_img).numpy())\n",
        "    plt.title(\"Noisy Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(2, 3, 6)\n",
        "    plt.imshow(tf.squeeze(denoised_img).numpy())\n",
        "    plt.title(\"Denoised Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amdHj5zZ4ghk"
      },
      "outputs": [],
      "source": [
        "# Test the network on a sample image\n",
        "result = test_network_batch(model, \"images/test/image_138.png\", noise_std=0.1)\n",
        "median = apply_median_filter(result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
