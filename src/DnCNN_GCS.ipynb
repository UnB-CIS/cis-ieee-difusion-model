{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0bD9OmC2qb1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "import gc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRbbbItte8Jt"
      },
      "outputs": [],
      "source": [
        "project_id = 'axial-glow-456914-n5'\n",
        "\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBjejGSZilc4"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "storage_client = storage.Client(project=project_id)\n",
        "\n",
        "bucket_name = 'cis-difusion-dataset'\n",
        "bucket = storage_client.bucket(bucket_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwO1VBkgSpKW"
      },
      "outputs": [],
      "source": [
        "def check_folder_contents(bucket, folder_name):\n",
        "    \"\"\"\n",
        "    Checks and lists files inside a specific folder in a GCS bucket.\n",
        "\n",
        "    Args:\n",
        "        bucket: The GCS bucket object\n",
        "        folder_name: The name of the folder to check\n",
        "\n",
        "    Returns:\n",
        "        A list of file names in the folder\n",
        "    \"\"\"\n",
        "    \n",
        "    if not folder_name.endswith('/'):\n",
        "        folder_name += '/'\n",
        "\n",
        "   \n",
        "    blobs = list(bucket.list_blobs(prefix=folder_name))\n",
        "\n",
        "    \n",
        "    files = [blob.name for blob in blobs if blob.name != folder_name]\n",
        "\n",
        "    \n",
        "    print(f\"Checking folder: {folder_name}\")\n",
        "\n",
        "    if files:\n",
        "        print(f\"Found {len(files)} files in the folder:\")\n",
        "        for file in files:\n",
        "            print(f\"- {file}\")\n",
        "    else:\n",
        "        print(f\"The folder '{folder_name}' is empty or doesn't exist.\")\n",
        "\n",
        "    return files\n",
        "\n",
        "\n",
        "check_folder_contents(bucket, \"DIV2K_train_HR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9mVmtiz8jtX7"
      },
      "outputs": [],
      "source": [
        "folder_name = 'DIV2K_train_HR'  \n",
        "local_path = '/dataset/'  \n",
        "\n",
        "def download_files_from_gcs(bucket_name, folder_name, local_path):\n",
        "    \"\"\"\n",
        "    Downloads files from a Google Cloud Storage folder to a local directory.\n",
        "    \"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    os.makedirs(local_path, exist_ok=True)\n",
        "\n",
        "    blobs = bucket.list_blobs(prefix=folder_name)\n",
        "    for blob in blobs:\n",
        "        if not blob.name.endswith('/'):\n",
        "            file_name = os.path.basename(blob.name)\n",
        "            blob.download_to_filename(os.path.join(local_path, file_name))\n",
        "            print(f'Downloaded: {blob.name} to {os.path.join(local_path, file_name)}')\n",
        "\n",
        "    print('Download complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOXiG41hvB54"
      },
      "outputs": [],
      "source": [
        "def download_image_batch(bucket, folder_name, local_path, batch_size=100, start_index=0):\n",
        "    \"\"\"Downloads a batch of images from Google Cloud Storage.\"\"\"\n",
        "    # Create local directory if it doesn't exist\n",
        "    os.makedirs(local_path, exist_ok=True)\n",
        "\n",
        "    blobs = list(bucket.list_blobs(prefix=folder_name))\n",
        "    image_paths = []\n",
        "    count = 0\n",
        "\n",
        "    for i, blob in enumerate(blobs):\n",
        "        if i < start_index:\n",
        "            continue  # Skip already downloaded images\n",
        "        if not blob.name.endswith('/') and count < batch_size:\n",
        "            file_name = os.path.basename(blob.name)\n",
        "            local_file_path = os.path.join(local_path, file_name)\n",
        "            blob.download_to_filename(local_file_path)\n",
        "            image_paths.append(local_file_path)  # Ensure this is a string\n",
        "            count += 1\n",
        "            print(f'Downloaded: {blob.name} to {local_file_path}')\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print(f'Downloaded {count} images.')\n",
        "    return image_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHFiYCSL2ioa"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(bucket_name, folder_name, noise_std=0.1, img_size=(128, 128), is_training=True):  # Added is_training flag\n",
        "    \"\"\"\n",
        "    Prepares the dataset for training by adding noise to the images.\n",
        "    \"\"\"\n",
        "    def load_and_preprocess_image(path):\n",
        "        # Load and preprocess image\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.image.decode_png(img, channels=3)\n",
        "        # Resize the image\n",
        "        img = tf.image.resize(img, img_size)\n",
        "        img = tf.cast(img, tf.float32) / 255.0  # Normalize to [0,1]\n",
        "\n",
        "        if is_training:\n",
        "            # Add Gaussian noise\n",
        "            noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=noise_std)\n",
        "            noisy_img = img + noise\n",
        "\n",
        "            # Clip values to keep between [0,1]\n",
        "            noisy_img = tf.clip_by_value(noisy_img, 0.0, 1.0)\n",
        "\n",
        "            return noisy_img, img  # Return noisy and original for training\n",
        "        else:\n",
        "            return img, img  # Return original twice for testing\n",
        "\n",
        "    # List paths of all images in the folder\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blobs = bucket.list_blobs(prefix=folder_name)\n",
        "    image_paths = [f'gs://{bucket_name}/{blob.name}' for blob in blobs if not blob.name.endswith('/')]\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(4)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TeD-PuyRI-M"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset_local(local_image_paths, noise_std=0.1, img_size=(128, 128), is_training=True):\n",
        "    \"\"\"\n",
        "    Prepara o dataset para treinamento usando imagens locais baixadas.\n",
        "    \"\"\"\n",
        "    # First, ensure all paths are strings\n",
        "    local_image_paths = [str(path) for path in local_image_paths]\n",
        "\n",
        "    def load_and_preprocess_image(path):\n",
        "        # Ensure path is a string (TensorFlow operations might convert it)\n",
        "        path = tf.convert_to_tensor(path, dtype=tf.string)\n",
        "\n",
        "        # Print the path and its data type for debugging\n",
        "        tf.print(\"Path type:\", tf.debugging.assert_type(path, tf.string))\n",
        "\n",
        "        # Carregar e pré-processar imagem\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.image.decode_png(img, channels=3)\n",
        "        # Redimensionar a imagem\n",
        "        img = tf.image.resize(img, img_size)\n",
        "        img = tf.cast(img, tf.float32) / 255.0  # Normalizar para [0,1]\n",
        "\n",
        "        if is_training:\n",
        "            # Adicionar ruído Gaussiano\n",
        "            noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=noise_std)\n",
        "            noisy_img = img + noise\n",
        "\n",
        "            # Limitar valores para manter entre [0,1]\n",
        "            noisy_img = tf.clip_by_value(noisy_img, 0.0, 1.0)\n",
        "\n",
        "            return noisy_img, img  # Retornar ruidosa e original para treinamento\n",
        "        else:\n",
        "            return img, img  # Retornar original duas vezes para teste\n",
        "\n",
        "    # Create dataset with explicit string type\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(tf.constant(local_image_paths, dtype=tf.string))\n",
        "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(4)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuDACNkY2W4Q"
      },
      "outputs": [],
      "source": [
        "class DnCNN(Model):\n",
        "    def __init__(self, D, C=64):\n",
        "        super(DnCNN, self).__init__()\n",
        "        self.D = D\n",
        "        # Create convolution layers\n",
        "        self.conv_layers = [layers.Conv2D(C, kernel_size=3, padding='same', input_shape=(None, None, 3))]\n",
        "        self.conv_layers.extend([layers.Conv2D(C, kernel_size=3, padding='same') for _ in range(D)])\n",
        "        self.conv_layers.append(layers.Conv2D(3, kernel_size=3, padding='same'))\n",
        "        # BatchNormalization doesn't take an activation parameter\n",
        "        self.bn_layers = [layers.BatchNormalization() for _ in range(D)]\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        h = tf.nn.relu(self.conv_layers[0](x))\n",
        "        for i in range(self.D):\n",
        "            # Apply batch normalization\n",
        "            h = self.bn_layers[i](self.conv_layers[i + 1](h), training=training)\n",
        "            # Apply ReLU activation separately\n",
        "            h = tf.nn.relu(h)\n",
        "        y = self.conv_layers[-1](h) + x\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5cDZdpS2e4a"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dataset, epochs=200, learning_rate=1e-3):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "    model.fit(train_dataset, epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya6pHczV1QSG"
      },
      "outputs": [],
      "source": [
        "def train_model_in_batches(model, bucket, train_folder_name, local_path, batch_size=100, epochs_per_batch=10):\n",
        "    \"\"\"Treina o modelo em batches de imagens baixadas do GCS, continuando de pesos anteriores se existirem.\"\"\"\n",
        "    num_batches = 5  # Você pode ajustar isso com base no número desejado de batches\n",
        "    \n",
        "    # Build the model before loading weights\n",
        "    # Create a dummy input tensor of the expected shape\n",
        "    dummy_input = tf.random.normal((1, 128, 128, 3))  # Adjust shape as needed\n",
        "    _ = model(dummy_input)  # This builds the model\n",
        "    \n",
        "    # Check if weights file exists and load it\n",
        "    weights_path = '/content/weights.weights.h5'\n",
        "    if os.path.exists(weights_path):\n",
        "        print(\"Loading existing weights to continue training...\")\n",
        "        model.load_weights(weights_path)\n",
        "    else:\n",
        "        print(\"No existing weights found. Training from scratch.\")\n",
        "    \n",
        "    # Initialize start_index to track our position in the dataset\n",
        "    start_index = 0\n",
        "    \n",
        "    for batch_index in range(num_batches):\n",
        "        print(f\"Treinando no batch {batch_index + 1}/{num_batches}\")\n",
        "        print(f\"Starting from image index: {start_index}\")\n",
        "        \n",
        "        # Clean the local directory before downloading new images\n",
        "        if os.path.exists(local_path):\n",
        "            shutil.rmtree(local_path)\n",
        "        os.makedirs(local_path, exist_ok=True)\n",
        "    \n",
        "\n",
        "        # Download and prepare a batch of images\n",
        "        image_paths = download_image_batch(bucket, \n",
        "                                          train_folder_name, \n",
        "                                          local_path, \n",
        "                                          batch_size=batch_size, \n",
        "                                          start_index=start_index)\n",
        "        \n",
        "        # Increment start_index for the next batch\n",
        "        start_index += batch_size\n",
        "        \n",
        "        train_dataset = prepare_dataset_local(image_paths)\n",
        "        \n",
        "        # Treinar o modelo para o batch atual\n",
        "        train_model(model, train_dataset, epochs=epochs_per_batch)\n",
        "        \n",
        "        # Salvar pesos após cada batch\n",
        "        model.save_weights(weights_path)\n",
        "        print(f\"Treinamento do batch {batch_index + 1} completo. Pesos salvos.\")\n",
        "        \n",
        "        # Cleanup to save memory\n",
        "        del train_dataset\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gkhyg7xZvlJo"
      },
      "outputs": [],
      "source": [
        "def test_network_batch(model, bucket_name, test_folder_name, noise_std=0.1, batch_size=1):\n",
        "    \"\"\"\n",
        "    Tests the model on a batch of images loaded from GCS.\n",
        "    Shows images at full resolution in a vertical layout.\n",
        "    \"\"\"\n",
        "    # Path for downloaded images\n",
        "    local_path = '/content/temp/test_images/'\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Load the saved weights into the model if they exist\n",
        "    if os.path.exists('/content/weights.weights.h5'):\n",
        "        model.load_weights('/content/weights.weights.h5')\n",
        "        print(\"Loaded saved weights for testing.\")\n",
        "    else:\n",
        "        print(\"No saved weights found. Using initialized weights.\")\n",
        "\n",
        "    # Ensure temp directory exists\n",
        "    os.makedirs(local_path, exist_ok=True)\n",
        "\n",
        "    # Download a single image for demonstration\n",
        "    blobs = list(storage_client.bucket(bucket_name).list_blobs(prefix=test_folder_name))\n",
        "    \n",
        "    # Filter out folder entries and take just one image\n",
        "    image_blobs = [blob for blob in blobs if not blob.name.endswith('/')]\n",
        "    \n",
        "    if not image_blobs:\n",
        "        print(f\"No images found in {test_folder_name}\")\n",
        "        return None\n",
        "        \n",
        "    # Just use the first image for demonstration\n",
        "    blob = image_blobs[0]\n",
        "    file_name = os.path.basename(blob.name)\n",
        "    local_file_path = os.path.join(local_path, file_name)\n",
        "    blob.download_to_filename(local_file_path)\n",
        "    print(f'Downloaded: {blob.name} to {local_file_path}')\n",
        "    \n",
        "    # Load the image at full resolution for display\n",
        "    img_highres = tf.io.read_file(local_file_path)\n",
        "    img_highres = tf.image.decode_png(img_highres, channels=3)\n",
        "    img_highres = tf.cast(img_highres, tf.float32) / 255.0\n",
        "    \n",
        "    # Store original dimensions\n",
        "    original_height, original_width = img_highres.shape[0], img_highres.shape[1]\n",
        "    print(f\"Original image dimensions: {original_height}x{original_width}\")\n",
        "    \n",
        "    # Create a resized version for the model\n",
        "    model_input_size = (128, 128)\n",
        "    img = tf.image.resize(img_highres, model_input_size)\n",
        "\n",
        "    # Create a copy for adding noise\n",
        "    img_for_noise = tf.identity(img)\n",
        "    \n",
        "    # Add noise to the model-size image\n",
        "    noise = tf.random.normal(shape=tf.shape(img_for_noise), mean=0.0, stddev=noise_std)\n",
        "    noisy_img = img_for_noise + noise\n",
        "    noisy_img = tf.clip_by_value(noisy_img, 0.0, 1.0)\n",
        "    \n",
        "    # Add batch dimension for the model\n",
        "    noisy_img_batch = tf.expand_dims(noisy_img, 0)\n",
        "    \n",
        "    # Process with the model\n",
        "    denoised_img_batch = model(noisy_img_batch, training=False)\n",
        "    denoised_img = tf.squeeze(denoised_img_batch)\n",
        "    \n",
        "    # Create a high-resolution noisy image for display\n",
        "    noise_highres = tf.random.normal(shape=tf.shape(img_highres), mean=0.0, stddev=noise_std)\n",
        "    noisy_img_highres = img_highres + noise_highres\n",
        "    noisy_img_highres = tf.clip_by_value(noisy_img_highres, 0.0, 1.0)\n",
        "    \n",
        "    # Resize the denoised image back to high resolution for comparison\n",
        "    denoised_img_highres = tf.image.resize(denoised_img, (original_height, original_width))\n",
        "    \n",
        "    # Display in vertical layout for better visualization\n",
        "    plt.figure(figsize=(10, 24))  # Vertical figure layout\n",
        "    \n",
        "    # Display original image\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.imshow(img_highres.numpy())\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "    \n",
        "    # Display noisy image\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.imshow(noisy_img_highres.numpy())\n",
        "    plt.title(\"Noisy Image\")\n",
        "    plt.axis(\"off\")\n",
        "    \n",
        "    # Display denoised image\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.imshow(denoised_img_highres.numpy())\n",
        "    plt.title(\"Denoised Image\")\n",
        "    plt.axis(\"off\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print PSNR values for comparison\n",
        "    noisy_psnr = tf.image.psnr(\n",
        "        img_highres,\n",
        "        noisy_img_highres,\n",
        "        max_val=1.0\n",
        "    )\n",
        "    denoised_psnr = tf.image.psnr(\n",
        "        img_highres,\n",
        "        denoised_img_highres,\n",
        "        max_val=1.0\n",
        "    )\n",
        "    \n",
        "    print(f\"Noisy Image PSNR: {noisy_psnr.numpy():.2f} dB\")\n",
        "    print(f\"Denoised Image PSNR: {denoised_psnr.numpy():.2f} dB\")\n",
        "    \n",
        "    # Clean up temporary files\n",
        "    shutil.rmtree(local_path, ignore_errors=True)\n",
        "    gc.collect()\n",
        "    \n",
        "    return {\n",
        "        'original': img_highres.numpy(),\n",
        "        'noisy': noisy_img_highres.numpy(),\n",
        "        'denoised': denoised_img_highres.numpy(),\n",
        "        'local_path': local_file_path\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU5SlYcm3zXM"
      },
      "outputs": [],
      "source": [
        "def test_network(model, image_path, noise_std=0.1):\n",
        "    # Load and preprocess the image\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.resize(img, (128, 128))\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    # Add Gaussian noise\n",
        "    noise = tf.random.normal(shape=tf.shape(img), mean=0.0, stddev=noise_std)\n",
        "    noisy_img = img + noise\n",
        "    noisy_img = tf.clip_by_value(noisy_img, 0.0, 1.0)  # Keep values in [0, 1]\n",
        "\n",
        "    # Predict denoised using the model\n",
        "    denoised_img = model(noisy_img, training=False)\n",
        "\n",
        "    # Denoise the image by subtracting the predicted noise\n",
        "    denoised_img = tf.clip_by_value(denoised_img, 0.0, 1.0)\n",
        "\n",
        "    # Visualize the results\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.imshow(tf.squeeze(img).numpy())\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.imshow(tf.squeeze(noisy_img).numpy())\n",
        "    plt.title(\"Noisy Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.imshow(tf.squeeze(denoised_img).numpy())\n",
        "    plt.title(\"Denoised Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "    return (img, noisy_img, denoised_img)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3ceUcSRf54Yl",
        "outputId": "2daab08c-fafc-4968-868a-3a35450f1c2c"
      },
      "outputs": [],
      "source": [
        "# Definir parâmetros\n",
        "model = DnCNN(D=8)\n",
        "train_folder_name = f\"{folder_name}/DIV2K_train_HR\"  # Nome da pasta de treinamento\n",
        "local_path = '/content/dataset/'  # Caminho local para salvar os dados baixados\n",
        "os.makedirs('/content/dataset/', exist_ok=True)\n",
        "batch_size = 100  # Tamanho do batch\n",
        "epochs_per_batch = 10  # Épocas por batch\n",
        "\n",
        "# Treinar o modelo em batches\n",
        "train_model_in_batches(model, bucket, train_folder_name, local_path, batch_size, epochs_per_batch)\n",
        "\n",
        "# Testar o modelo\n",
        "test_folder_name = f\"{folder_name}/DIV2K_valid_HR\"  # Nome da pasta de teste\n",
        "result = test_network_batch(model, bucket_name, test_folder_name, noise_std=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testar o modelo\n",
        "test_folder_name = \"DIV2K_valid_HR\"  # Nome da pasta de teste\n",
        "test_network_batch(model, bucket_name, test_folder_name, noise_std=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_AMEwXaIeE8"
      },
      "outputs": [],
      "source": [
        "def apply_median_filter(result, kernel_size=3):\n",
        "    \"\"\"\n",
        "    Apply median filter to the noisy image and display comparison of original, noisy, and denoised images.\n",
        "    \"\"\"\n",
        "    # Extract images from result\n",
        "    denoised_img = result[0]  # First is denoised image\n",
        "    noisy_img = result[1]     # Second is noisy image\n",
        "    original_img = result[2]  # Third is original image\n",
        "\n",
        "    # Convert TensorFlow tensor to numpy array - remove batch dimension\n",
        "    denoised_img_np = tf.squeeze(denoised_img).numpy()\n",
        "    noisy_img_np = tf.squeeze(noisy_img).numpy()\n",
        "    original_img_np = tf.squeeze(original_img).numpy()\n",
        "\n",
        "    # Apply median filter\n",
        "    median_filtered = np.zeros_like(noisy_img_np)\n",
        "    for i in range(3):  # Apply filter to each channel independently\n",
        "        median_filtered[:, :, i] = cv2.medianBlur(noisy_img_np[:, :, i], kernel_size)\n",
        "\n",
        "    # Create a figure for display\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Display original image\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(original_img_np)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Display noisy image\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(noisy_img_np)\n",
        "    plt.title(\"Noisy Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Display denoised image\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(denoised_img_np)\n",
        "    plt.title(\"Denoised Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print PSNR values for comparison\n",
        "    noisy_psnr = tf.image.psnr(\n",
        "        tf.convert_to_tensor(original_img_np),\n",
        "        tf.convert_to_tensor(noisy_img_np),\n",
        "        max_val=1.0\n",
        "    )\n",
        "    denoised_psnr = tf.image.psnr(\n",
        "        tf.convert_to_tensor(original_img_np),\n",
        "        tf.convert_to_tensor(denoised_img_np),\n",
        "        max_val=1.0\n",
        "    )\n",
        "\n",
        "    print(f\"Noisy Image PSNR: {noisy_psnr.numpy():.2f} dB\")\n",
        "    print(f\"Denoised Image PSNR: {denoised_psnr.numpy():.2f} dB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amdHj5zZ4ghk"
      },
      "outputs": [],
      "source": [
        "# Test the network on a sample image\n",
        "gcs_image_path = \"DIV2K_valid_HR/0801.png\"  # Or another image in your bucket\n",
        "result = test_single_image(model, bucket, gcs_image_path, noise_std=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_single_image(model, gcs_bucket, image_path, noise_std=0.1):\n",
        "    \"\"\"Tests the model on a single image from GCS.\"\"\"\n",
        "\n",
        "    # Load the saved weights into the model\n",
        "    if os.path.exists('/content/weights.weights.h5'):\n",
        "        model.load_weights('/content/weights.weights.h5')\n",
        "        print(\"Loaded saved weights for testing.\")\n",
        "\n",
        "    # Create temp directory\n",
        "    temp_dir = '/content/temp/'\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    # Download the image from GCS\n",
        "    blob = gcs_bucket.blob(image_path)\n",
        "    local_path = os.path.join(temp_dir, os.path.basename(image_path))\n",
        "    blob.download_to_filename(local_path)\n",
        "    print(f\"Downloaded test image to {local_path}\")\n",
        "\n",
        "    # Now process the local file\n",
        "    img = tf.io.read_file(local_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    img = tf.image.resize(img, (128, 128))\n",
        "\n",
        "    # Create a separate copy for adding noise\n",
        "    img_for_noise = tf.identity(img)\n",
        "\n",
        "    # Add noise\n",
        "    noise = tf.random.normal(shape=tf.shape(img_for_noise), mean=0.0, stddev=noise_std)\n",
        "    noisy_img = img_for_noise + noise\n",
        "    noisy_img = tf.clip_by_value(noisy_img, 0.0, 1.0)\n",
        "\n",
        "    # Add batch dimension\n",
        "    noisy_img = tf.expand_dims(noisy_img, 0)\n",
        "    img = tf.expand_dims(img, 0)\n",
        "\n",
        "    # Denoise the image\n",
        "    denoised_img = model(noisy_img, training=False)\n",
        "\n",
        "    return denoised_img, noisy_img, img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Then, apply the median filter and visualize the results\n",
        "apply_median_filter(result, kernel_size=3)  # kernel_size is optional, default is 3"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
