{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2df2ad3-df8f-403d-b558-c961a3169220",
   "metadata": {},
   "source": [
    "# Primeiro rascunho das funções do modelo de difusão (PREPARAÇÃO DAS IMAGENS E TESTES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e965d473-b4d0-4f0c-81cc-a165bf9e7958",
   "metadata": {},
   "source": [
    "## Importações e testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ebd8f-6a26-4fe6-8f52-8b7cf4b11dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 19:35:41.559092: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-12 19:35:41.559782: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-12 19:35:41.562368: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-12 19:35:41.568697: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731440141.578992    1152 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731440141.582021    1152 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-12 19:35:41.592954: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-12 19:35:42.868532: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "\n",
    "# desativar erros de Cuda (verificar versão do tensorflow utilizada)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c33209-2eca-4d81-a690-3e4939a94dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n",
      "tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# testar tensforflow:\n",
    "print(tf.__version__)\n",
    "tensor = tf.constant([1, 2, 3, 4, 5])\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a2df0-66aa-45f2-a4df-2db77b870b1f",
   "metadata": {},
   "source": [
    "## 1. Carregar dataset\n",
    "\n",
    "Aqui estamos assumindo que o conjunto de imagens vem como imagem + rotulo. O rotulo da imagem é o próprio nome do arquivo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fcd33e7-993f-4fca-a3a8-04e20f2efe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir constantes (rascunho)\n",
    "IMG_SIZE = 256  \n",
    "BATCH_SIZE = 32  \n",
    "TRAIN_SPLIT = 0.8  \n",
    "VAL_SPLIT = 0.2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b170bb6f-7ac3-47a9-b46a-51d9a29f3a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image, label):\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    \n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e017c-5757-4c3b-9bdb-3ec8ded07b33",
   "metadata": {},
   "source": [
    "### load_and_preprocess_image: Redimensiona e normaliza uma imagem. Retorna a imagem normalizada e o label/rotulo\n",
    "    \n",
    "- Parâmetros:\n",
    "    - image: A imagem a ser processada.\n",
    "    - label: O label associado à imagem.\n",
    "\n",
    "- (Resize): função tf.cast(image, tf.float32): Converte os valores de pixel da imagem para float. Geralmente, imagens carregadas de arquivos têm valores de pixel inteiros entre 0 e 255. Para redes neurais, aparentemente trabalhar com float32 é a maneira mais comum.\n",
    "\n",
    "- (Normalize): Divisão por 255.0 (normalização): Após a conversão, cada valor de pixel é dividido por 255.0 para ajustar o intervalo dos valores de pixel de [0, 255] para [0, 1]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3926952f-1a70-4494-a132-716b5af7f3f3",
   "metadata": {},
   "source": [
    "## 2 Divisão em Conjuntos de Treinamento, Validação e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1c4036b-6a35-4d48-a0d6-30d019bfec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_split=TRAIN_SPLIT, val_split=VAL_SPLIT):\n",
    "    # Calcular o número total de exemplos\n",
    "    total_examples = dataset.cardinality().numpy()\n",
    "    \n",
    "    # Calcular o número de exemplos para cada conjunto\n",
    "    train_size = int(total_examples * train_split)\n",
    "    val_size = int(total_examples * val_split)\n",
    "    \n",
    "    # Dividir o dataset\n",
    "    train_ds = dataset.take(train_size)\n",
    "    remaining_ds = dataset.skip(train_size)\n",
    "    val_ds = remaining_ds.take(val_size)\n",
    "    \n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f494ff-7279-492b-a4a9-b74dab7113de",
   "metadata": {},
   "source": [
    "### load_and_preprocess_image: Divide o conjunto de dados em treinamento, validação e teste. Retorna os conjuntos de treino e validação\n",
    "\n",
    "- Parâmetros:\n",
    "    - dataset: O conjunto de dados completo.\n",
    "    - train_split: Proporção de dados para treinamento.\n",
    "    - val_split: Proporção de dados para validação.\n",
    "\n",
    "total_examples: A primeira coisa que a função faz é calcular o número total de exemplos presentes no dataset usando a propriedade .cardinality(), que retorna o número de elementos no tf.data.Dataset. Esse valor é convertido para numpy() para ser manipulado como um número inteiro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda6c29-4686-499a-b932-b22a5f7265ce",
   "metadata": {},
   "source": [
    "## 3. Carregar e Pré-processar a Base de Dados, Dividir Conjuntos e Preparar Pipeline para Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35153e92-16bf-4985-be5a-5384a5a13e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch_size=BATCH_SIZE):\n",
    "    # Carregar o dataset CIFAR-10 (apenas dados de treino)\n",
    "    dataset, info = tfds.load('cifar10', split='train', as_supervised=True, with_info=True)\n",
    "    \n",
    "    # Aplicar pré-processamento em cada exemplo do dataset\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Dividir o dataset\n",
    "    train_ds, val_ds = split_dataset(dataset)\n",
    "    \n",
    "    # Configurar o conjunto de treinamento e validação com embaralhamento, batch e prefetching -  estudar casos de uso e importância\n",
    "    train_ds = train_ds.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c00ea-ba67-4537-9dbd-392f446dabb8",
   "metadata": {},
   "source": [
    "### prepare_dataset: Prepara o dataset para o treinamento, incluindo carregamento, pré-processamento e divisão. Retorna o dataset de treino e o de validação\n",
    "    \n",
    "- Parâmetros:\n",
    "    - batch_size: Tamanho do batch.\n",
    "\n",
    "- shuffle: Garante que o modelo não aprenda padrões artificiais devido à ordem dos dados. Se os dados forem processados na ordem em que foram carregados, pode haver correlação entre exemplos consecutivos (como, por exemplo, uma sequência de imagens de uma única classe). Isso pode levar a overfitting ou a um treinamento menos eficaz.\n",
    "\n",
    "- batching: Divide o dataset em pequenos batches ou lotes de dados. Cada lote contém um número fixo de exemplos, que são processados juntos durante a fase de treinamento. O batching é essencial para permitir que o modelo seja treinado em grandes volumes de dados sem sobrecarregar a memória.\n",
    "\n",
    "- prefetch: Carrega os próximos batches de dados enquanto o modelo ainda está treinando no batch atual. Isso garante que o modelo tenha dados prontos para o próximo passo de treinamento sem esperar pelo carregamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a2dfab-96ae-4ee9-9044-e564ab434d45",
   "metadata": {},
   "source": [
    "## 4*. Testar funções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a82a34",
   "metadata": {},
   "source": [
    "1.  Testar implementação da função load_and_preprocess_image com nomes genericos\n",
    "2. Testar implementação para nomes que representam mais detalhadamente a imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d445f1-c474-42fd-aa42-c7627109ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames = [\"cats_00001.png\", \"cats_00002.png\", \"cats_00003.png\",\n",
    "                   \"dogs_00001.png\", \"dogs_00002\", \"dogs_00003\",\n",
    "                   \"panda_00001.png\", \"panda_00002.png\", \"panda_00003.png\"] \n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(image_filenames)\n",
    "# dataset = dataset.map(lambda x: load_image_from_name(x), num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087446b0-d588-48ae-8f9c-71180918f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = prepare_dataset()\n",
    "\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(\"Batch de imagens (forma):\", images.shape)\n",
    "    print(\"Labels correspondentes:\", labels.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
